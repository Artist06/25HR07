{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74177a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import whois\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_internal_link(link, base_domain):\n",
    "    parsed = urlparse(link)\n",
    "    return parsed.netloc == '' or base_domain in parsed.netloc\n",
    "\n",
    "def is_null_link(link):\n",
    "    if not link:\n",
    "        return True\n",
    "    href = link.strip().lower()\n",
    "    return (\n",
    "        href == '#' or\n",
    "        href.startswith('#') or\n",
    "        href.startswith('javascript:') or\n",
    "        href in ('javascript::void(0)', 'javascript:void(0)', '/')\n",
    "    )\n",
    "\n",
    "def safe_request(url):\n",
    "    \"\"\"Perform a GET request safely, returning response or None.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, allow_redirects=True)\n",
    "        return response\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_domain(url):\n",
    "    \"\"\"Extract domain from a URL (without scheme).\"\"\"\n",
    "    return urlparse(url).netloc\n",
    "\n",
    "\n",
    "def calculate_features(url, soup, with_redirect=False):\n",
    "    \"\"\"\n",
    "    Calculate web features like number of internal/external links,\n",
    "    CSS, redirects, errors, favicon, and null links.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"total_links\": 0,\n",
    "        \"internal_links\": 0,\n",
    "        \"external_links\": 0,\n",
    "        \"null_links\": 0,\n",
    "        \"internal_css\": 0,\n",
    "        \"external_css\": 0,\n",
    "        \"internal_errors\": 0,\n",
    "        \"external_errors\": 0,\n",
    "        \"internal_favicon\": 0,\n",
    "        \"external_favicon\": 0\n",
    "    }\n",
    "\n",
    "    if with_redirect:\n",
    "        features.update({\n",
    "            \"internal_redirects\": 0,\n",
    "            \"external_redirects\": 0\n",
    "        })\n",
    "\n",
    "    base_domain = extract_domain(url)\n",
    "\n",
    "    # 1. Hyperlinks\n",
    "    links = [a.get(\"href\") for a in soup.find_all(\"a\", href=True)]\n",
    "    features[\"total_links\"] = len(links)\n",
    "\n",
    "    for link in links:\n",
    "        try:\n",
    "            if is_null_link(link):\n",
    "                features[\"null_links\"] += 1\n",
    "                continue\n",
    "\n",
    "            full_link = urljoin(url, link)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if is_internal_link(full_link, base_domain):\n",
    "            features[\"internal_links\"] += 1\n",
    "        else:\n",
    "            features[\"external_links\"] += 1\n",
    "\n",
    "        # optional redirect/error checks\n",
    "        if with_redirect:\n",
    "            response = safe_request(full_link)\n",
    "            if response:\n",
    "                if len(response.history) > 0:  # redirected\n",
    "                    if is_internal_link(response.url, base_domain):\n",
    "                        features[\"internal_redirects\"] += 1\n",
    "                    else:\n",
    "                        features[\"external_redirects\"] += 1\n",
    "                if response.status_code >= 400:  # error\n",
    "                    if is_internal_link(full_link, base_domain):\n",
    "                        features[\"internal_errors\"] += 1\n",
    "                    else:\n",
    "                        features[\"external_errors\"] += 1\n",
    "\n",
    "    #  2. CSS\n",
    "    css_links = [link.get(\"href\") for link in soup.find_all(\"link\", rel=\"stylesheet\") if link.get(\"href\")]\n",
    "    for css in css_links:\n",
    "        full_css = urljoin(url, css)\n",
    "        if is_internal_link(full_css, base_domain):\n",
    "            features[\"internal_css\"] += 1\n",
    "        else:\n",
    "            features[\"external_css\"] += 1\n",
    "\n",
    "    #3. Favicon\n",
    "    favicons = [\n",
    "        link.get(\"href\")\n",
    "        for link in soup.find_all(\"link\", rel=lambda v: v and \"icon\" in v)\n",
    "        if link.get(\"href\")\n",
    "    ]\n",
    "    for icon in favicons:\n",
    "        full_icon = urljoin(url, icon)\n",
    "        if is_internal_link(full_icon, base_domain):\n",
    "            features[\"internal_favicon\"] += 1\n",
    "        else:\n",
    "            features[\"external_favicon\"] += 1\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def main(url, soup, with_redirect=False, log=False):\n",
    "    features = calculate_features(url, soup, with_redirect)\n",
    "    if log:\n",
    "        print(f\"Feature summary for {url}:\")\n",
    "        for key, value in features.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_message(message : str):\n",
    "    print('*'*len(message))\n",
    "    print(message)\n",
    "    print('*'*len(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e90ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_folder = './doms/'\n",
    "output_folder = './hyperlinks_without_redirects/'\n",
    "for i in range(8):\n",
    "    input_filename = f'dom_data{i}.pkl'\n",
    "\n",
    "    message = f\"Extracting hyperlink data from {input_filename}\"\n",
    "    decorate_message(message)\n",
    "\n",
    "    print(f\"Reading {input_filename}\")\n",
    "    start = time.time()\n",
    "    with open(dom_folder+input_filename, 'rb') as dom_file:\n",
    "        doms = pickle.load(dom_file)\n",
    "    print(f\"{input_filename} read, Time taken: {time.time() - start:.2f}s\")\n",
    "    \n",
    "    output = doms[:]\n",
    "    \n",
    "    beginning = start\n",
    "    total_doms = len(output)\n",
    "    start = time.time()\n",
    "\n",
    "    for index in range(total_doms):\n",
    "        output[index][2] = main(output[index][1], output[index][2], True)\n",
    "        end = time.time()\n",
    "        if (index%1000 == 0 and index) or (end - start) > 30:\n",
    "            print(f\"Current Progress: {index}/{total_doms}, Time Taken: {end - start:.2f}s\")\n",
    "            start = time.time()\n",
    "\n",
    "    filename = f'hyperlink_data{i}.pkl'\n",
    "    print(f\"Saving {input_filename}'s hyperlink data to file: {filename}\")\n",
    "\n",
    "    with open(output_folder+filename, 'wb') as output_file:\n",
    "        pickle.dump(output, output_file)\n",
    "\n",
    "    print(f\"Total time taken: {time.time() - beginning:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink_folder = output_folder = './hyperlinks_without_redirects/'\n",
    "hyperlink_df = pd.DataFrame()\n",
    "\n",
    "for i in range(8):\n",
    "    input_filename = f'hyperlink_data{i}.pkl'\n",
    "\n",
    "    with open(hyperlink_folder+input_filename, 'rb') as hyperlink_file:\n",
    "        hyperlink_data = pickle.load(hyperlink_file)\n",
    "        \n",
    "    current_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                'index': row[0],\n",
    "                'url': row[1],\n",
    "                **row[2],\n",
    "                'label': row[3]\n",
    "            }\n",
    "            for row in hyperlink_data\n",
    "        ]\n",
    "    ).set_index('index')\n",
    "    hyperlink_df = pd.concat([hyperlink_df, current_df], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2808b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink_df.to_csv(output_folder+'hyperlink_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
