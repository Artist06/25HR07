{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e52c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Bidirectional, LSTM,\n",
    "    GRU, Dense, Dropout, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcf3edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos_to_wordnet(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def clean_and_lemmatize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+|[^a-z\\s]', ' ', text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    for word, tag in tagged:\n",
    "        wn_tag = nltk_pos_to_wordnet(tag)\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag) if wn_tag else lemmatizer.lemmatize(word)\n",
    "        lemmatized.append(lemma)\n",
    "\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "def word_parse(soup):\n",
    "    title_tag = soup.find('title')\n",
    "    title_text = title_tag.get_text(separator=' ', strip=True) if title_tag else \"\"\n",
    "\n",
    "    body_tag = soup.find('body')\n",
    "    body_text = body_tag.get_text(separator=' ', strip=True) if body_tag else \"\"\n",
    "\n",
    "    parsed_title = clean_and_lemmatize(title_text)\n",
    "    parsed_content = clean_and_lemmatize(body_text)\n",
    "\n",
    "    return parsed_title, parsed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792406e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_message(message : str):\n",
    "    print('*'*len(message))\n",
    "    print(message)\n",
    "    print('*'*len(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fd9482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse0.pkl already computed\n",
      "parse1.pkl already computed\n",
      "parse2.pkl already computed\n",
      "parse3.pkl already computed\n",
      "parse4.pkl already computed\n",
      "parse5.pkl already computed\n",
      "parse6.pkl already computed\n",
      "parse7.pkl already computed\n"
     ]
    }
   ],
   "source": [
    "dom_folder = './doms/'\n",
    "\n",
    "for i in range(8):\n",
    "    input_filename = f'dom_data{i}.pkl'\n",
    "    output_folder = './parse/'\n",
    "    output_filename = f'parse{i}.pkl'\n",
    "    \n",
    "    if os.path.exists(output_folder+output_filename):\n",
    "        print(f\"{output_filename} already computed\")\n",
    "        continue\n",
    "    \n",
    "    decorate_message(f\"Parsing and Lemmatization for: {input_filename}\")\n",
    "    \n",
    "    beginning = start = time.time()\n",
    "    print(f\"Reading from {input_filename}\")\n",
    "    with open(dom_folder+input_filename, 'rb') as dom_file:\n",
    "        dom = pickle.load(dom_file)\n",
    "    print(f\"Time taken: {time.time() - start:.2f}s\")\n",
    "        \n",
    "    output = dom[:]\n",
    "    total_data = len(output)\n",
    "    start = time.time()\n",
    "    \n",
    "    for index in range(total_data):\n",
    "        parsed_title, parsed_content = word_parse(output[index][2])\n",
    "        \n",
    "        output[index].append(output[index][3])\n",
    "        output[index][2] = parsed_title\n",
    "        output[index][3] = parsed_content\n",
    "        \n",
    "        end = time.time()\n",
    "        if (index%1000 == 0 and index) or end - start > 30:\n",
    "            print(f\"Current Progress: {index}/{total_data}, Time taken: {end - start:.2f}\")\n",
    "            start = time.time()\n",
    "        \n",
    "    \n",
    "    print(f\"Writing to {output_filename}\")\n",
    "    start = time.time()\n",
    "    with open(output_folder+output_filename, 'wb') as output_file:\n",
    "        pickle.dump(output, output_file)\n",
    "    print(f\"Time taken: {time.time() - start:.2f}\")\n",
    "    print(f\"Total time taken: {time.time() - beginning:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51baedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_titles = []\n",
    "page_contents = []\n",
    "\n",
    "for i in range(8):\n",
    "    with open(f'./parse/parse{i}.pkl', 'rb') as parse_file:\n",
    "        parse_list = pickle.load(parse_file)\n",
    "    \n",
    "    for item in parse_list:\n",
    "        page_titles.append(item[2])\n",
    "        page_contents.append(item[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(page_titles, page_contents, max_title_len=10, max_content_len=100, vocab_size=10000):\n",
    "    all_texts = page_titles + page_contents\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "    title_seq = tokenizer.texts_to_sequences(page_titles)\n",
    "    content_seq = tokenizer.texts_to_sequences(page_contents)\n",
    "\n",
    "    title_pad = pad_sequences(title_seq, maxlen=max_title_len, padding='post', truncating='post')\n",
    "    content_pad = pad_sequences(content_seq, maxlen=max_content_len, padding='post', truncating='post')\n",
    "\n",
    "    return tokenizer, title_pad, content_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0d64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, title_pad, content_pad = prepare_inputs(page_titles, page_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588a5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = './input_data/'\n",
    "\n",
    "if not os.path.exists(input_data_path+'tokenizer.pkl'):\n",
    "    with open(input_data_path+'tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "        pickle.dump(tokenizer, tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8cf4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path=\"glove.6B.100d.txt\"):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Loaded {len(embeddings_index):,} word vectors from GloVe.\")\n",
    "    return embeddings_index\n",
    "\n",
    "def build_embedding_matrix(tokenizer, embeddings_index, embedding_dim=100):\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(\"Embedding matrix shape:\", embedding_matrix.shape)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d68e4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400,000 word vectors from GloVe.\n",
      "Embedding matrix shape: (747931, 100)\n"
     ]
    }
   ],
   "source": [
    "glove_index = load_glove_embeddings(\"glove.6B.100d.txt\")\n",
    "embedding_matrix = build_embedding_matrix(tokenizer, glove_index, embedding_dim=100)\n",
    "\n",
    "if not os.path.exists(input_data_path+'embedding_matrix.pkl'):\n",
    "    with open(input_data_path+'embedding_matrix.pkl', 'wb') as embedding_matrix_file:\n",
    "        pickle.dump(embedding_matrix, embedding_matrix_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b648fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "\n",
    "for i in range(8):\n",
    "    input_filename = f'parse{i}.pkl'\n",
    "    \n",
    "    with open('./parse/'+input_filename, 'rb') as parse_file:\n",
    "        parse_list = pickle.load(parse_file)\n",
    "        \n",
    "    input_data.extend(parse_list)\n",
    "\n",
    "for index in range(len(input_data)):\n",
    "    input_data[index][2] = title_pad[index]\n",
    "    input_data[index][3] = content_pad[index]\n",
    "\n",
    "if not os.path.exists(input_data_path+'input_data.pkl'):\n",
    "    with open(input_data_path+'input_data.pkl', 'wb') as input_data_file:\n",
    "        pickle.dump(input_data, input_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89ca884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [data[4] for data in input_data]\n",
    "labels = np.array(labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "071b9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bilstm_model(vocab_size, embedding_dim, max_title_len, max_content_len, embedding_matrix):\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=None,\n",
    "        trainable=False,\n",
    "        name=\"shared_embedding\"\n",
    "    )\n",
    "\n",
    "    # Inputs\n",
    "    title_input = Input(shape=(max_title_len,), name=\"title_input\")\n",
    "    content_input = Input(shape=(max_content_len,), name=\"content_input\")\n",
    "\n",
    "    # Shared embedding + BiLSTM\n",
    "    title_emb = embedding_layer(title_input)\n",
    "    content_emb = embedding_layer(content_input)\n",
    "\n",
    "    title_bilstm = Bidirectional(LSTM(64))(title_emb)\n",
    "    content_bilstm = Bidirectional(LSTM(128))(content_emb)\n",
    "\n",
    "    # Merge both\n",
    "    merged = Concatenate()([title_bilstm, content_bilstm])\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    merged = Dense(128, activation='relu')(merged)\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(inputs=[title_input, content_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c62139",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = build_bilstm_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=100,\n",
    "    max_title_len=10,\n",
    "    max_content_len=100,\n",
    "    embedding_matrix=embedding_matrix\n",
    ")\n",
    "if not os.path.exists('./model/BiLSTM.pkl'):\n",
    "    model.fit(\n",
    "        {\"title_input\": title_pad, \"content_input\": content_pad},\n",
    "        labels,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb63f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./model/BiLSTM.pkl'):\n",
    "    with open('./model/BiLSTM.pkl', 'wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "else:\n",
    "    with open('./model/BiLSTM.pkl', 'rb') as model_file:\n",
    "        model = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d32a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 7ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = model.predict({\n",
    "    \"title_input\": title_pad,\n",
    "    \"content_input\": content_pad\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd5efbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    \"url\":[data[1] for data in input_data],\n",
    "    \"predicted_label\": [round(data[0]) for data in predicted_labels],\n",
    "    \"actual_label\": [data[4] for data in input_data]\n",
    "})\n",
    "\n",
    "predictions_df.to_csv(\"./model/content_bilstm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9639fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shared_embedding(vocab_size, embedding_dim, embedding_matrix):\n",
    "    return Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=None,\n",
    "        trainable=False,\n",
    "        name=\"shared_embedding\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "531b7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(vocab_size, embedding_dim, max_title_len, max_content_len, embedding_matrix):\n",
    "    embedding_layer = create_shared_embedding(vocab_size, embedding_dim, embedding_matrix)\n",
    "\n",
    "    # Inputs\n",
    "    title_input = Input(shape=(max_title_len,), name=\"title_input\")\n",
    "    content_input = Input(shape=(max_content_len,), name=\"content_input\")\n",
    "\n",
    "    # Embedding + LSTM\n",
    "    title_emb = embedding_layer(title_input)\n",
    "    content_emb = embedding_layer(content_input)\n",
    "\n",
    "    title_lstm = LSTM(128)(title_emb)\n",
    "    content_lstm = LSTM(256)(content_emb)\n",
    "\n",
    "    merged = Concatenate()([title_lstm, content_lstm])\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    merged = Dense(128, activation='relu')(merged)\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(inputs=[title_input, content_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a7b95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model(vocab_size, embedding_dim, max_title_len, max_content_len, embedding_matrix):\n",
    "    embedding_layer = create_shared_embedding(vocab_size, embedding_dim, embedding_matrix)\n",
    "\n",
    "    title_input = Input(shape=(max_title_len,), name=\"title_input\")\n",
    "    content_input = Input(shape=(max_content_len,), name=\"content_input\")\n",
    "\n",
    "    title_emb = embedding_layer(title_input)\n",
    "    content_emb = embedding_layer(content_input)\n",
    "\n",
    "    title_gru = GRU(128)(title_emb)\n",
    "    content_gru = GRU(256)(content_emb)\n",
    "\n",
    "    merged = Concatenate()([title_gru, content_gru])\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    merged = Dense(128, activation='relu')(merged)\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(inputs=[title_input, content_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efe52ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigru_model(vocab_size, embedding_dim, max_title_len, max_content_len, embedding_matrix):\n",
    "    embedding_layer = create_shared_embedding(vocab_size, embedding_dim, embedding_matrix)\n",
    "\n",
    "    title_input = Input(shape=(max_title_len,), name=\"title_input\")\n",
    "    content_input = Input(shape=(max_content_len,), name=\"content_input\")\n",
    "\n",
    "    title_emb = embedding_layer(title_input)\n",
    "    content_emb = embedding_layer(content_input)\n",
    "\n",
    "    title_bigru = Bidirectional(GRU(64))(title_emb)\n",
    "    content_bigru = Bidirectional(GRU(128))(content_emb)\n",
    "\n",
    "    merged = Concatenate()([title_bigru, content_bigru])\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    merged = Dense(128, activation='relu')(merged)\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(inputs=[title_input, content_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dc1da90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 28ms/step - accuracy: 0.8785 - loss: 0.2971 - val_accuracy: 0.9188 - val_loss: 0.2081\n",
      "Epoch 2/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 28ms/step - accuracy: 0.9245 - loss: 0.2010 - val_accuracy: 0.9371 - val_loss: 0.1724\n",
      "Epoch 3/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 29ms/step - accuracy: 0.9374 - loss: 0.1677 - val_accuracy: 0.9371 - val_loss: 0.1636\n",
      "Epoch 4/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 35ms/step - accuracy: 0.9460 - loss: 0.1463 - val_accuracy: 0.9430 - val_loss: 0.1588\n",
      "Epoch 5/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 37ms/step - accuracy: 0.9515 - loss: 0.1312 - val_accuracy: 0.9445 - val_loss: 0.1564\n",
      "Epoch 6/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 29ms/step - accuracy: 0.9569 - loss: 0.1166 - val_accuracy: 0.9464 - val_loss: 0.1535\n",
      "Epoch 7/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 36ms/step - accuracy: 0.9621 - loss: 0.1062 - val_accuracy: 0.9498 - val_loss: 0.1530\n",
      "Epoch 8/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 29ms/step - accuracy: 0.9651 - loss: 0.0965 - val_accuracy: 0.9538 - val_loss: 0.1424\n",
      "Epoch 9/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 29ms/step - accuracy: 0.9667 - loss: 0.0914 - val_accuracy: 0.9525 - val_loss: 0.1510\n",
      "Epoch 10/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 29ms/step - accuracy: 0.9702 - loss: 0.0823 - val_accuracy: 0.9544 - val_loss: 0.1736\n"
     ]
    }
   ],
   "source": [
    "model = build_lstm_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=100,\n",
    "    max_title_len=10,\n",
    "    max_content_len=100,\n",
    "    embedding_matrix=embedding_matrix\n",
    ")\n",
    "if not os.path.exists('./model/LSTM.pkl'):\n",
    "    model.fit(\n",
    "        {\"title_input\": title_pad, \"content_input\": content_pad},\n",
    "        labels,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfa0f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./model/LSTM.pkl'):\n",
    "    with open('./model/LSTM.pkl', 'wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "else:\n",
    "    with open('./model/LSTM.pkl', 'rb') as model_file:\n",
    "        model = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a4104a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    \"url\":[data[1] for data in input_data],\n",
    "    \"predicted_label\": [round(data[0]) for data in predicted_labels],\n",
    "    \"actual_label\": [data[4] for data in input_data]\n",
    "})\n",
    "\n",
    "predictions_df.to_csv(\"./model/content_lstm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a24b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 28ms/step - accuracy: 0.8706 - loss: 0.3047 - val_accuracy: 0.9295 - val_loss: 0.1859\n",
      "Epoch 2/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 29ms/step - accuracy: 0.9349 - loss: 0.1791 - val_accuracy: 0.9405 - val_loss: 0.1618\n",
      "Epoch 3/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 30ms/step - accuracy: 0.9471 - loss: 0.1462 - val_accuracy: 0.9491 - val_loss: 0.1463\n",
      "Epoch 4/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 41ms/step - accuracy: 0.9572 - loss: 0.1196 - val_accuracy: 0.9524 - val_loss: 0.1401\n",
      "Epoch 5/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 41ms/step - accuracy: 0.9660 - loss: 0.0982 - val_accuracy: 0.9579 - val_loss: 0.1367\n",
      "Epoch 6/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 30ms/step - accuracy: 0.9724 - loss: 0.0803 - val_accuracy: 0.9569 - val_loss: 0.1467\n",
      "Epoch 7/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 40ms/step - accuracy: 0.9764 - loss: 0.0702 - val_accuracy: 0.9576 - val_loss: 0.1647\n",
      "Epoch 8/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 39ms/step - accuracy: 0.9790 - loss: 0.0601 - val_accuracy: 0.9568 - val_loss: 0.1651\n",
      "Epoch 9/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 39ms/step - accuracy: 0.9801 - loss: 0.0579 - val_accuracy: 0.9585 - val_loss: 0.1783\n",
      "Epoch 10/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 31ms/step - accuracy: 0.9809 - loss: 0.0543 - val_accuracy: 0.9604 - val_loss: 0.1892\n"
     ]
    }
   ],
   "source": [
    "model = build_gru_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=100,\n",
    "    max_title_len=10,\n",
    "    max_content_len=100,\n",
    "    embedding_matrix=embedding_matrix\n",
    ")\n",
    "if not os.path.exists('./model/GRU.pkl'):\n",
    "    model.fit(\n",
    "        {\"title_input\": title_pad, \"content_input\": content_pad},\n",
    "        labels,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5cbef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./model/GRU.pkl'):\n",
    "    with open('./model/GRU.pkl', 'wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "else:\n",
    "    with open('./model/GRU.pkl', 'rb') as model_file:\n",
    "        model = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "081d4f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 14ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = model.predict({\n",
    "    \"title_input\": title_pad,\n",
    "    \"content_input\": content_pad\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2de0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    \"url\":[data[1] for data in input_data],\n",
    "    \"predicted_label\": [round(data[0]) for data in predicted_labels],\n",
    "    \"actual_label\": [data[4] for data in input_data]\n",
    "})\n",
    "\n",
    "predictions_df.to_csv(\"./model/content_gru_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d153279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 30ms/step - accuracy: 0.8815 - loss: 0.2798 - val_accuracy: 0.9406 - val_loss: 0.1611\n",
      "Epoch 2/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 28ms/step - accuracy: 0.9431 - loss: 0.1591 - val_accuracy: 0.9472 - val_loss: 0.1488\n",
      "Epoch 3/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 30ms/step - accuracy: 0.9539 - loss: 0.1310 - val_accuracy: 0.9549 - val_loss: 0.1351\n",
      "Epoch 4/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 32ms/step - accuracy: 0.9633 - loss: 0.1043 - val_accuracy: 0.9542 - val_loss: 0.1327\n",
      "Epoch 5/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 31ms/step - accuracy: 0.9689 - loss: 0.0886 - val_accuracy: 0.9577 - val_loss: 0.1306\n",
      "Epoch 6/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 32ms/step - accuracy: 0.9730 - loss: 0.0765 - val_accuracy: 0.9592 - val_loss: 0.1339\n",
      "Epoch 7/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 33ms/step - accuracy: 0.9757 - loss: 0.0680 - val_accuracy: 0.9590 - val_loss: 0.1491\n",
      "Epoch 8/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 33ms/step - accuracy: 0.9776 - loss: 0.0630 - val_accuracy: 0.9594 - val_loss: 0.1624\n",
      "Epoch 9/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 33ms/step - accuracy: 0.9809 - loss: 0.0572 - val_accuracy: 0.9594 - val_loss: 0.1730\n",
      "Epoch 10/10\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 33ms/step - accuracy: 0.9803 - loss: 0.0559 - val_accuracy: 0.9582 - val_loss: 0.2023\n"
     ]
    }
   ],
   "source": [
    "model = build_bigru_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=100,\n",
    "    max_title_len=10,\n",
    "    max_content_len=100,\n",
    "    embedding_matrix=embedding_matrix\n",
    ")\n",
    "if not os.path.exists('./model/BiGRU.pkl'):\n",
    "    model.fit(\n",
    "        {\"title_input\": title_pad, \"content_input\": content_pad},\n",
    "        labels,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58361517",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./model/BiGRU.pkl'):\n",
    "    with open('./model/BiGRU.pkl', 'wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "else:\n",
    "    with open('./model/BiGRU.pkl', 'rb') as model_file:\n",
    "        model = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "422f7b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 10ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = model.predict({\n",
    "    \"title_input\": title_pad,\n",
    "    \"content_input\": content_pad\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2419b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    \"url\":[data[1] for data in input_data],\n",
    "    \"predicted_label\": [round(data[0]) for data in predicted_labels],\n",
    "    \"actual_label\": [data[4] for data in input_data]\n",
    "})\n",
    "\n",
    "predictions_df.to_csv(\"./model/content_bigru_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
